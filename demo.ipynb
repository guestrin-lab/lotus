{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8a9d553-49bc-42e3-8b56-c53ec83eb78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "Requirement already satisfied: nest-asyncio in /opt/homebrew/anaconda3/envs/lotus/lib/python3.10/site-packages (1.6.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/opt/homebrew/anaconda3/envs/lotus/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nest-asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d132d52e-aecf-4fb4-9f36-9305e870f402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/lejiang/sources/spark-3.5.4/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/lejiang/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/lejiang/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c4b5f016-c5ce-40ec-8f52-921f210aaedb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.0-SNAPSHOT in local-m2-cache\n",
      ":: resolution report :: resolve 56ms :: artifacts dl 1ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.0-SNAPSHOT from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c4b5f016-c5ce-40ec-8f52-921f210aaedb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/3ms)\n",
      "25/01/16 16:56:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Initialize Spark Session with the custom spark runtime jar that contains the spark action for embedding\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OpenHouse-Example\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.0-SNAPSHOT\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", f\"{os.getcwd()}/warehouse\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", f\"{os.getcwd()}/local/warehouse\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224082e7-7477-40a4-a6a0-3dcc4a2e252f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/lotus/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/anaconda3/envs/lotus/lib/python3.10/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "2025-01-16 16:57:06,122 - INFO - Use pytorch device: mps\n"
     ]
    }
   ],
   "source": [
    "from openhouse_connector import SparkConnector, OpenHouse\n",
    "\n",
    "# Create `employees` table in Spark\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS db\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS db.employees\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS db.employees (id INT, name STRING, age INT) USING iceberg\")\n",
    "\n",
    "# Initialize OpenHouse with SparkConnector\n",
    "openhouse = OpenHouse(SparkConnector(spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aeb7c8f-aaf3-404d-bc80-bf8a5a9087f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample data as a pandas DataFrame\n",
    "data = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3, 4, 5],\n",
    "    \"name\": [\"Emma\", \"Michael\", \"Sophia\", \"James\", \"Olivia\"],\n",
    "    \"age\": [27, 33, 29, 45, 31]\n",
    "})\n",
    "\n",
    "data2 = pd.DataFrame({\n",
    "    \"id\": [6, 7, 8, 9, 10],\n",
    "    \"name\": [\"Lisa\", \"Omar\", \"Nina\", \"Peter\", \"Rachel\"],\n",
    "    \"age\": [31, 45, 28, 39, 33]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d71b675a-6fca-41f6-8037-a2ff72bcfc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 17:02:02,549 - INFO - Error while sending or receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/lotus/lib/python3.10/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "2025-01-16 17:02:02,609 - INFO - Closing down clientserver connection\n",
      "2025-01-16 17:02:02,610 - INFO - Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/lotus/lib/python3.10/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/lotus/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/homebrew/anaconda3/envs/lotus/lib/python3.10/site-packages/py4j/clientserver.py\", line 506, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending\n",
      "2025-01-16 17:02:02,646 - INFO - Closing down clientserver connection\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1420.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id    name  age\n",
      "0   1    Emma   27\n",
      "4   5  Olivia   31\n"
     ]
    }
   ],
   "source": [
    "# Write to table and search the content with semantic query\n",
    "table = openhouse.table(\"db.employees\")\n",
    "table.write(data, index_cols=[\"name\"])\n",
    "employees = table.head(5)\n",
    "outdf = employees.sem_search(\"name\", \"Who's emma?\", K=2, n_rerank=4)\n",
    "print(outdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf8266-63ba-41f7-a2df-cfd7759a1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
